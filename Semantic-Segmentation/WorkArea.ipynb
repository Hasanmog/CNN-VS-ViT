{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from dataloader import WHU\n",
    "from utils import dataset_split\n",
    "from torchvision import transforms\n",
    "from model import Segmentor\n",
    "from utils import calculate_iou\n",
    "from engine import train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset into train/test/val --> 280/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../Datasets/WHU-Bldg/dataset/train\"\n",
    "val_dir = \"../Datasets/WHU-Bldg/dataset/val\"\n",
    "test_dir = \"../Datasets/WHU-Bldg/dataset/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the effect of downsizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4\n",
    "# Open the original image\n",
    "image = Image.open(os.path.join(train_dir,f\"Images/{index}.png\")).convert(\"RGB\")\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((512, 512))  # Use a smaller size to see the downsizing effect\n",
    "])\n",
    "\n",
    "# Apply the transformation\n",
    "image_2 = Image.open(os.path.join(train_dir,f\"Masks/{index}.png\"))\n",
    "\n",
    "image = np.array(image)\n",
    "image_2 = np.array(image_2)\n",
    "\n",
    "\n",
    "fig , axs = plt.subplots(nrows= 1 , ncols=2 , figsize = (15,15))\n",
    "axs[0].imshow(image)\n",
    "axs[0].set_title(\"Original Image\")\n",
    "axs[0].set_axis_off()\n",
    "axs[1].imshow(image_2 , cmap = 'gray')\n",
    "axs[1].set_title(\"Mask\")\n",
    "axs[1].set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "BATCH_SIZE = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.01\n",
    "scheduler = \"exponential\"\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),          # Randomly crop images to 224x224\n",
    "    transforms.RandomHorizontalFlip(),          # Random horizontal flipping\n",
    "    transforms.RandomRotation(10),              # Random rotation of +/- 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly jitter color\n",
    "    transforms.ToTensor(),                      # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])    # Normalize for pretrained models\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WHU(parent_dir=train_dir)\n",
    "\n",
    "val_dataset = WHU(parent_dir=val_dir)\n",
    "\n",
    "test_dataset = WHU(parent_dir=test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "len(train_loader) , len(test_loader) , len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(test_loader)\n",
    "batch = next(val_iter)\n",
    "images , masks = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 8 # sample number from the batch\n",
    "mask = masks[index].squeeze()\n",
    "image = images[index]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.unsqueeze(dim = 0).permute(1 , 2 , 0)\n",
    "mask = mask.numpy()\n",
    "img = image.permute(1 , 2 , 0)\n",
    "img = img.numpy()\n",
    "\n",
    "fig , axs = plt.subplots(nrows = 1 , ncols = 2 , figsize = (10 , 10))\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title(\"Original Image\")\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(mask)\n",
    "axs[1].set_title(\"Mask\")\n",
    "axs[1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmentor().to(device)\n",
    "summary(model , input_size=( 3 , 512 , 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss , val_loss  = train_val(model = model , \n",
    "                                                        train_loader = train_loader, \n",
    "                                                        val_loader = val_loader, \n",
    "                                                        epochs = epochs, \n",
    "                                                        lr = lr, \n",
    "                                                        device = device , \n",
    "                                                        lr_schedule=scheduler , \n",
    "                                                        out_dir = \"weights/resume/best_checkpoint.pth\" , \n",
    "                                                        neptune_config=\"../neptune.json\" , \n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"weights/best_checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "image = image.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    outputs = model(image.to(device))\n",
    "\n",
    "\n",
    "# Assuming `output_probs` is your model's output tensor with shape [1, 1, H, W]\n",
    "# and contains probability values between 0 and 1 from a sigmoid activation\n",
    "threshold = 0.5  # Commonly used threshold\n",
    "binary_mask = (outputs > threshold).float()  # Convert to 0 or 1 based on the threshold\n",
    "# Convert PyTorch tensor to numpy array\n",
    "binary_mask = binary_mask.to('cpu')\n",
    "binary_mask_np = binary_mask.numpy().squeeze()  # Squeeze out channel and batch dimensions if necessary\n",
    "\n",
    "# Morphological closing to fill small holes\n",
    "kernel = np.ones((3,3),np.uint8)  # You can adjust kernel size based on your needs\n",
    "closed_mask = cv2.morphologyEx(binary_mask_np, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Morphological opening to remove noise\n",
    "cleaned_mask = cv2.morphologyEx(closed_mask, cv2.MORPH_OPEN, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = outputs.squeeze()\n",
    "# Convert the tensor to a numpy array for visualization\n",
    "# predicted_labels = outputs.cpu().numpy()\n",
    "image = image.squeeze().permute(1 , 2 , 0)\n",
    "image = image.cpu().numpy()\n",
    "# Convert the mask to a color image\n",
    "# Visualize using matplotlib\n",
    "fig , axs = plt.subplots(nrows = 1 , ncols=3 , figsize = (20 , 20))\n",
    "axs[0].imshow(image)\n",
    "axs[0].set_title(\"Image\")\n",
    "axs[0].set_axis_off()\n",
    "axs[1].imshow(mask , cmap='viridis')\n",
    "axs[1].set_title(\"gt mask\")\n",
    "axs[1].set_axis_off()\n",
    "axs[2].imshow(cleaned_mask , cmap = 'viridis')\n",
    "axs[2].set_title(\"predicted mask\")\n",
    "axs[2].set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
