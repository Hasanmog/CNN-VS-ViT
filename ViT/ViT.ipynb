{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of the paper Image is Worth 16x16 Words: Transformers for Image Recognition at Scale in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms , datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../datasets/pizza_steak_sushi/train\"\n",
    "test_dir = \"../datasets/pizza_steak_sushi/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../datasets/pizza_steak_sushi/train/pizza/5764.jpg\")\n",
    "print(\"image_size\" , image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size , img_size)) , \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#create datasets\n",
    "train_data = datasets.ImageFolder(\n",
    "    \"../datasets/pizza_steak_sushi/train\" , \n",
    "    transform = transform\n",
    ")\n",
    "\n",
    "test_data = datasets.ImageFolder(\n",
    "    \"../datasets/pizza_steak_sushi/test\" , \n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset = train_data , \n",
    "    batch_size = batch_size , \n",
    "    shuffle = True , \n",
    "    num_workers = 4 , \n",
    "    pin_memory= True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = test_data , \n",
    "    batch_size = batch_size , \n",
    "    shuffle = False , \n",
    "    num_workers = 4 , \n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "train_dataloader , test_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of images\n",
    "image_batch , label_batch = next(iter(train_dataloader))\n",
    "print(\"image_batch\" , image_batch.shape)\n",
    "print(\"label_batch\" , label_batch.shape)\n",
    "# get single image from batch\n",
    "image , label = image_batch[0] , label_batch[0]\n",
    "\n",
    "image.shape , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for image in image_batch:\n",
    "    image = image.permute(1 , 2 , 0)\n",
    "    img.append(image)\n",
    "    \n",
    "grid_size = int(np.ceil(np.sqrt(batch_size))) \n",
    "#np.ceil --> rounding\n",
    "fig, axs = plt.subplots(grid_size , grid_size , figsize=(20, 20))\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        ax = axs[i , j ]\n",
    "        if i * grid_size +j <batch_size:\n",
    "            ax.imshow(img[i * grid_size+ j], cmap='gray' , aspect='auto')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "plt.subplots_adjust(wspace = 0.1  , hspace= 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images are 224 x 224 x 3\n",
    "height = 224 # H\n",
    "width = 224 # W\n",
    "channels = 3 # C\n",
    "patch_size = 16 # P\n",
    "\n",
    "numb_patches = int((height * width) / patch_size**2)\n",
    "print(f\"Number of Patches (N) with image resolution {height}x{width} is {numb_patches} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_input_shape = (height , width , channels)\n",
    "\n",
    "# 196 patches each of size 16 x 16\n",
    "# output will be for each image --> 192 patches , P**2 * C\n",
    "embedding_layer_output_shape = (numb_patches , patch_size**2 * channels) \n",
    "\n",
    "print(f\"Input Image shape : {embedding_layer_input_shape}\")\n",
    "print(f\"Output embedded shape flattened to patches : {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image , label = image_batch[5] , label_batch[5]\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(classes[label])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing top row patched pixels\n",
    "\n",
    "permuted_image = image.permute(1,2,0) # H W C\n",
    "\n",
    "plt.figure(figsize=(patch_size , patch_size))\n",
    "plt.imshow(permuted_image[:patch_size , : , :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224 \n",
    "patch_size = 16\n",
    "num_patches = img_size / patch_size\n",
    "\n",
    "assert img_size % patch_size ==0 , \"image size must be divisible by patch_size\"\n",
    "print(f\"number of patches per row: {num_patches}\\nPatch size is {patch_size} x {patch_size} pixels\")\n",
    "\n",
    "fig,axs = plt.subplots(nrows = 1, \n",
    "                       ncols = img_size // patch_size ,\n",
    "                       figsize = (num_patches , num_patches) , \n",
    "                       sharex = True , \n",
    "                       sharey = True)\n",
    "\n",
    "for i,patch in enumerate(range(0 , img_size , patch_size)): #start from zero move by patch size in img_size\n",
    "    axs[i].imshow(permuted_image[:patch_size , patch:patch+patch_size , :])\n",
    "    axs[i].set_xlabel(i+1)\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size / patch_size\n",
    "\n",
    "assert img_size % patch_size  == 0 , \"the img size should be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "fig , axs = plt.subplots(\n",
    "    nrows = img_size // patch_size , \n",
    "    ncols = img_size // patch_size , \n",
    "    figsize = (num_patches , num_patches),\n",
    "    sharex = True , \n",
    "    sharey = True\n",
    ")\n",
    "\n",
    "for i , patch_height in enumerate(range(0 , img_size ,patch_size )):\n",
    "   for j , patch_width in enumerate(range(0 , img_size , patch_size)):\n",
    "       \n",
    "    axs[i , j].imshow(permuted_image[patch_height:patch_height+patch_size , \n",
    "                                        patch_width:patch_width+patch_size , :])\n",
    "    axs[i, j].set_ylabel(i+1,\n",
    "                             rotation=\"horizontal\",\n",
    "                             horizontalalignment=\"right\",\n",
    "                             verticalalignment=\"center\")\n",
    "    axs[i, j].set_xlabel(j+1)\n",
    "    axs[i, j].set_xticks([])\n",
    "    axs[i, j].set_yticks([])\n",
    "    axs[i, j].label_outer()\n",
    "        \n",
    "fig.suptitle(f\"{classes[label]} -> Patchified\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "embed_dim = 768 #D: number of feature / activation maps\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "    in_channels = 3 , \n",
    "    out_channels = embed_dim , \n",
    "    kernel_size = patch_size , \n",
    "    stride = patch_size , \n",
    "    padding = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(classes[label])\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image shape before: \" , image.shape)\n",
    "# expected shape for the Conv2d is (N , C , H , W)\n",
    "image = image.unsqueeze(0) \n",
    "print(\"image shape after\" , image.shape)\n",
    "\n",
    "img_conv = conv2d(image)\n",
    "print(\"image shape after Conv2D : \" , img_conv.shape)\n",
    "# output shape [batch_size , embed_dim , feature_map_height , feature_map_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random_indices = random.sample(range(0 , 758) , k=10)\n",
    "\n",
    "print(f\"showing random convolutional feature maps from indices : {random_indices}\" )\n",
    "\n",
    "fig , axs = plt.subplots(nrows = 1 , ncols=len(random_indices) , figsize=(12,12))\n",
    "\n",
    "for i,idx in enumerate(random_indices):\n",
    "    image_conv_feat_map = img_conv[: , idx , : , :]\n",
    "    \n",
    "    axs[i].imshow(image_conv_feat_map.squeeze().detach().numpy())\n",
    "    # detach takes a copy of the tensor that disconnected from the gradient graph \n",
    "    axs[i].set(xticklabels = [] , yticklabels = [] , xticks = [] ,\n",
    "               yticks = []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After turning the image into patch embedding , its time to flatten it\n",
    "\n",
    "print(f\"shape of the output of the conv : {img_conv.shape} -> [batch , embedding_dim , feature_map_height, feature_map_width]\")\n",
    "# what we want to flatten is the spatial dimension of the feature map\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "t = flatten(img_conv)\n",
    "print(t.shape) # this is flattening the whole tensor (1 , 768 * 14 * 14)\n",
    "\n",
    "flatten = nn.Flatten(start_dim= 2 , end_dim = 3 )\n",
    "t = flatten(img_conv)\n",
    "\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image , label = image_batch[5] , label_batch[5]\n",
    "\n",
    "plt.imshow(image.permute(1 , 2, 0))\n",
    "plt.title(classes[label])\n",
    "plt.axis(False)\n",
    "print(f\"original image shape: {image.shape}\")\n",
    "\n",
    "image_conv = conv2d(image.unsqueeze(0)) # (N , C, H , W)\n",
    "print(f\"shape after convolution: {image_conv.shape}\")# 768 feature maps each of size 14 x 14\n",
    "\n",
    "image_flatten = flatten(image_conv)\n",
    "print(f\"shape after flatten: {image_flatten.shape}\") # 768 feature maps each of flattend size of 196\n",
    "\n",
    "print(\"desired shape : N x(P^2 * C) --> (196 , 768)\")\n",
    "\n",
    "image_final = image_flatten.permute(0 , 2 , 1)\n",
    "print(f\"Patch embedding final shape : {image_final.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample = image_final[: , : , 0]\n",
    "print(\"shape of this sample : \" , single_sample.shape) # 2D image to 1D embedding vector\n",
    "plt.figure(figsize=(20 , 20))\n",
    "plt.imshow(single_sample.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {image_final.shape}\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up everything till now in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    turns 2d input image into a 1D sequence learnable embedding vector\n",
    "    \n",
    "    Args:\n",
    "    in_channels(int) : Number of channels in the input image. (Default set to 3)\n",
    "    patch_size (int) : size of the patch (Defaults to 16)\n",
    "    embed_dim (int) : number of feature maps (Defaults to 768) \n",
    "    \"\"\"\n",
    "    def __init__(self ,\n",
    "                 in_channels: int = 3 ,\n",
    "                 patch_size : int = 16 , \n",
    "                 embed_dim: int = 768 ):\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels= in_channels , \n",
    "                           out_channels = embed_dim , \n",
    "                           kernel_size= patch_size , \n",
    "                           stride = patch_size , \n",
    "                           padding = 0)\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim = 2 , end_dim = 3)\n",
    "    \n",
    "    def forward(self , image):\n",
    "        height = image.shape[-1]\n",
    "        assert height % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {height}, patch size: {patch_size}\"\n",
    "        patched_image = self.conv2d(image)\n",
    "        flattened_image = self.flatten(patched_image)\n",
    "        \n",
    "        return flattened_image.permute(0 , 2 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embed_dim=768)\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will give error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input sizes\n",
    "random_input_image = (1, 3, 224, 224)\n",
    "random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n",
    "\n",
    "# Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n",
    "summary(PatchEmbedding(),\n",
    "        input_size=random_input_image, # try swapping this for \"random_input_image_error\"\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"patch_embedded_image shape \" , patch_embedded_image.shape)\n",
    "\n",
    "batch_size , embedding_dim = patch_embedded_image.shape[0] , patch_embedded_image.shape[-1]\n",
    "\n",
    "class_token = nn.Parameter(torch.ones((batch_size , 1 , embedding_dim)) ,\n",
    "                           requires_grad= True)# [batch_size, number_of_tokens, embedding_dimension]\n",
    "\n",
    "print(f\"class_token: {class_token[: , : , :10]}\")\n",
    "print(f\"class_token shape: {class_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_patch = torch.cat((class_token , patch_embedded_image) , \n",
    "                          dim = 1)\n",
    "\n",
    "print(f\"shape of updated patch embedding with class token: {updated_patch.shape}\")\n",
    "\n",
    "# before each feature map was represented with 196 tokens  , however now after adding the class token each will be represented by 197\n",
    "\n",
    "print(updated_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embedding\n",
    "\n",
    "Epos --> (N+1 X D) where N is the number of patches and the 1 is for class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_patch , updated_patch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_patches = int((height * width) / patch_size **2)\n",
    "\n",
    "embedding_dim = updated_patch.shape[2]\n",
    "\n",
    "position_embedding = nn.Parameter(\n",
    "    torch.ones( 1 , numb_patches +1 , embedding_dim),\n",
    "                        requires_grad= True        ) # learnable\n",
    "\n",
    "position_embedding[: , :10 , :10] , position_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_with_position = updated_patch + position_embedding\n",
    "patch_with_position , patch_with_position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full code \n",
    "\n",
    "patch_size = 16\n",
    "\n",
    "image , label = image_batch[6] , label_batch[6]\n",
    "\n",
    "print(f\"image tensor shape : {image.shape}\")\n",
    "height , width = image.shape[1] , image.shape[2]\n",
    "\n",
    "x = image.unsqueeze(0) # add batch dim\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "patch_embedding_layer = PatchEmbedding(in_channels = image.shape[0] ,\n",
    "                                       patch_size = patch_size , \n",
    "                                       embed_dim = 768)\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size , 1 , embedding_dimension),\n",
    "                           requires_grad= True)\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "patch_embedding_with_class = torch.cat((class_token , patch_embedding) , dim = 1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_with_class.shape}\")\n",
    "\n",
    "# position embedding\n",
    "\n",
    "number_of_patches = int((height*width) / patch_size**2)\n",
    "\n",
    "position_embedding = nn.Parameter(torch.ones(1 , number_of_patches + 1 , embedding_dim),\n",
    "                                  requires_grad=True)\n",
    "\n",
    "patch_and_position_embedd = patch_embedding_with_class + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedd.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer encoder: \n",
    "\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int= 768 , \n",
    "                 n_heads: int=12 , \n",
    "                 attn_dropout: float = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim = embedding_dim , \n",
    "            num_heads = n_heads , \n",
    "            dropout = attn_dropout , \n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "    def forward(self , x):\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        attn_output , _ = self.multihead_attn(query = x , \n",
    "                                              value = x , \n",
    "                                              key = x , \n",
    "                                              need_weights = False)\n",
    "        \n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_self_attention_block = MultiheadSelfAttentionBlock(\n",
    "    embedding_dim = 768 , \n",
    "    n_heads= 12\n",
    ")\n",
    "\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedd)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedd.shape}\")\n",
    "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer norm -> linear layer -> non-linear layer -> dropout -> linear layer -> dropout\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int=768 , \n",
    "                 mlp_size: int = 3072 , # from paper\n",
    "                 dropout: float = 0.1): #from paper\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = embedding_dim , \n",
    "                      out_features = mlp_size) , \n",
    "            nn.GELU() , \n",
    "            nn.Dropout(p = dropout) , \n",
    "            nn.Linear(in_features = mlp_size , \n",
    "                      out_features = embedding_dim) , \n",
    "            nn.Dropout(p = dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self , x):\n",
    "        x = self.layer_norm(x)\n",
    "        mlp = self.mlp(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = MLPBlock(embedding_dim = 768 , \n",
    "                     mlp_size = 3072 , \n",
    "                     dropout = 0.1)\n",
    "\n",
    "mlp_out = mlp_block(patched_image_through_msa_block)\n",
    "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
    "print(f\"Output shape MLP block: {mlp_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMER ENCODER : \n",
    "# x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self , \n",
    "                 embedding_dim: int = 768 , \n",
    "                 num_heads: int = 12 , \n",
    "                 mlp_size : int = 3072 , \n",
    "                 mlp_dropout : float = 0.1 , \n",
    "                 attn_dropout : float = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(\n",
    "            embedding_dim= embedding_dim , \n",
    "            n_heads = num_heads , \n",
    "            attn_dropout= attn_dropout\n",
    "        )\n",
    "        self.mlp_block = MLPBlock(\n",
    "            embedding_dim= 768 , \n",
    "            mlp_size = mlp_size , \n",
    "            dropout = mlp_dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self , x):\n",
    "        \n",
    "        msa = self.msa_block(x) + x\n",
    "        \n",
    "        mlp = self.mlp_block(msa) + msa \n",
    "        \n",
    "        return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an instance of TransformerEncoderBlock\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "# Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer encoder with pytorch\n",
    "\n",
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model = 768 , \n",
    "    nhead = 12 , \n",
    "    dim_feedforward = 3072 , \n",
    "    dropout = 0.1 , \n",
    "    activation= 'gelu' , \n",
    "    batch_first= True , \n",
    "    norm_first = True\n",
    ")\n",
    "\n",
    "torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n",
    "summary(model=torch_transformer_encoder_layer,\n",
    "        input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL ViT code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit(nn.Module):\n",
    "    def __init__(self, \n",
    "                 img_size:int=224 ,\n",
    "                 in_channels:int=3 , \n",
    "                 embedding_dim:int=768,\n",
    "                 patch_size:int=16 , \n",
    "                 num_transformer_layers:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0,\n",
    "                 mlp_dropout:float=0.1 , \n",
    "                 embedding_dropout:float=0.1 , \n",
    "                 num_classes:int=1000):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "        self.class_embed = nn.Parameter(data = torch.randn(1 , 1 , embedding_dim),\n",
    "                                        requires_grad=True)\n",
    "        self.positional_embed = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        self.patch_embedding = PatchEmbedding(in_channels = in_channels , \n",
    "                                              patch_size = patch_size , \n",
    "                                              embed_dim = embedding_dim)\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(\n",
    "            embedding_dim= embedding_dim , \n",
    "            num_heads= num_heads , \n",
    "            mlp_size= mlp_size , \n",
    "            mlp_dropout= mlp_dropout\n",
    "        )for _ in range(num_transformer_layers)])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim , \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self , x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        class_token = self.class_embed.expand(batch_size, -1, -1)\n",
    "        \n",
    "        patches = self.patch_embedding(x)\n",
    "        \n",
    "        patches_with_cls = torch.cat((class_token , patches) , dim =1)\n",
    "        \n",
    "        patch_with_position = patches_with_cls + self.positional_embed\n",
    "        \n",
    "        embed_drop = self.embedding_dropout(patch_with_position)\n",
    "        \n",
    "        transformer = self.transformer_encoder(embed_drop)\n",
    "        \n",
    "        classes = self.classifier(transformer[: , 0])\n",
    "        \n",
    "        return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating the class embedding and expanding over a batch dimension\n",
    "batch_size = 32\n",
    "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n",
    "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
    "\n",
    "# Print out the change in shapes\n",
    "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n",
    "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a random tensor with same shape as a single image\n",
    "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
    "\n",
    "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = Vit(num_classes=len(classes))\n",
    "\n",
    "# Pass the random image tensor to our ViT instance\n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train\n",
    "\n",
    "vit = Vit()\n",
    "optimizer = torch.optim.Adam(\n",
    "    params = vit.parameters() , \n",
    "    lr = 3e-3 , \n",
    "    betas = (0.9 , 0.999) , \n",
    "    weight_decay = 0.3\n",
    "\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "results = train(model = vit , \n",
    "                train_dataloader= train_dataloader , \n",
    "                test_dataloader = test_dataloader , \n",
    "                optimizer = optimizer , \n",
    "                loss_fn= loss_fn , \n",
    "                epochs = 10 , \n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = results['train_loss']\n",
    "test_loss = results['test_loss']\n",
    "\n",
    "accuracy = results['train_acc']\n",
    "test_acc = results['test_acc']\n",
    "\n",
    "epochs = range(len(results['train_loss']))\n",
    "\n",
    "plt.figure(figsize=(15 , 7))\n",
    "\n",
    "plt.subplot(1 , 2 , 1)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, label=\"train_loss\")\n",
    "plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1 ,  2 , 2)\n",
    "plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
    "plt.plot(epochs, test_acc, label=\"test_accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained ViT from pytorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT #default --> best available\n",
    "\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights = pretrained_vit_weights , progress=True).to(device)\n",
    "\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "    \n",
    "pretrained_vit.heads = nn.Linear(in_features= 768 , out_features=len(classes)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
