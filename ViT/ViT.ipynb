{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of the paper Image is Worth 16x16 Words: Transformers for Image Recognition at Scale in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms , datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../datasets/pizza_steak_sushi/train\"\n",
    "test_dir = \"../datasets/pizza_steak_sushi/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../datasets/pizza_steak_sushi/train/pizza/5764.jpg\")\n",
    "print(\"image_size\" , image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size , img_size)) , \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#create datasets\n",
    "train_data = datasets.ImageFolder(\n",
    "    \"../datasets/pizza_steak_sushi/train\" , \n",
    "    transform = transform\n",
    ")\n",
    "\n",
    "test_data = datasets.ImageFolder(\n",
    "    \"../datasets/pizza_steak_sushi/test\" , \n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset = train_data , \n",
    "    batch_size = batch_size , \n",
    "    shuffle = True , \n",
    "    num_workers = 4 , \n",
    "    pin_memory= True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = test_data , \n",
    "    batch_size = batch_size , \n",
    "    shuffle = False , \n",
    "    num_workers = 4 , \n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "train_dataloader , test_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of images\n",
    "image_batch , label_batch = next(iter(train_dataloader))\n",
    "print(\"image_batch\" , image_batch.shape)\n",
    "print(\"label_batch\" , label_batch.shape)\n",
    "# get single image from batch\n",
    "image , label = image_batch[0] , label_batch[0]\n",
    "\n",
    "image.shape , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for image in image_batch:\n",
    "    image = image.permute(1 , 2 , 0)\n",
    "    img.append(image)\n",
    "    \n",
    "grid_size = int(np.ceil(np.sqrt(batch_size))) \n",
    "#np.ceil --> rounding\n",
    "fig, axs = plt.subplots(grid_size , grid_size , figsize=(20, 20))\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        ax = axs[i , j ]\n",
    "        if i * grid_size +j <batch_size:\n",
    "            ax.imshow(img[i * grid_size+ j], cmap='gray' , aspect='auto')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "plt.subplots_adjust(wspace = 0.1  , hspace= 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images are 224 x 224 x 3\n",
    "height = 224 # H\n",
    "width = 224 # W\n",
    "channels = 3 # C\n",
    "patch_size = 16 # P\n",
    "\n",
    "numb_patches = int((height * width) / patch_size**2)\n",
    "print(f\"Number of Patches (N) with image resolution {height}x{width} is {numb_patches} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_input_shape = (height , width , channels)\n",
    "\n",
    "# 196 patches each of size 16 x 16\n",
    "# output will be for each image --> 192 patches , P**2 * C\n",
    "embedding_layer_output_shape = (numb_patches , patch_size**2 * channels) \n",
    "\n",
    "print(f\"Input Image shape : {embedding_layer_input_shape}\")\n",
    "print(f\"Output embedded shape flattened to patches : {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image , label = image_batch[5] , label_batch[5]\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(classes[label])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing top row patched pixels\n",
    "\n",
    "permuted_image = image.permute(1,2,0) # H W C\n",
    "\n",
    "plt.figure(figsize=(patch_size , patch_size))\n",
    "plt.imshow(permuted_image[:patch_size , : , :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224 \n",
    "patch_size = 16\n",
    "num_patches = img_size / patch_size\n",
    "\n",
    "assert img_size % patch_size ==0 , \"image size must be divisible by patch_size\"\n",
    "print(f\"number of patches per row: {num_patches}\\nPatch size is {patch_size} x {patch_size} pixels\")\n",
    "\n",
    "fig,axs = plt.subplots(nrows = 1, \n",
    "                       ncols = img_size // patch_size ,\n",
    "                       figsize = (num_patches , num_patches) , \n",
    "                       sharex = True , \n",
    "                       sharey = True)\n",
    "\n",
    "for i,patch in enumerate(range(0 , img_size , patch_size)): #start from zero move by patch size in img_size\n",
    "    axs[i].imshow(permuted_image[:patch_size , patch:patch+patch_size , :])\n",
    "    axs[i].set_xlabel(i+1)\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size / patch_size\n",
    "\n",
    "assert img_size % patch_size  == 0 , \"the img size should be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "fig , axs = plt.subplots(\n",
    "    nrows = img_size // patch_size , \n",
    "    ncols = img_size // patch_size , \n",
    "    figsize = (num_patches , num_patches),\n",
    "    sharex = True , \n",
    "    sharey = True\n",
    ")\n",
    "\n",
    "for i , patch_height in enumerate(range(0 , img_size ,patch_size )):\n",
    "   for j , patch_width in enumerate(range(0 , img_size , patch_size)):\n",
    "       \n",
    "    axs[i , j].imshow(permuted_image[patch_height:patch_height+patch_size , \n",
    "                                        patch_width:patch_width+patch_size , :])\n",
    "    axs[i, j].set_ylabel(i+1,\n",
    "                             rotation=\"horizontal\",\n",
    "                             horizontalalignment=\"right\",\n",
    "                             verticalalignment=\"center\")\n",
    "    axs[i, j].set_xlabel(j+1)\n",
    "    axs[i, j].set_xticks([])\n",
    "    axs[i, j].set_yticks([])\n",
    "    axs[i, j].label_outer()\n",
    "        \n",
    "fig.suptitle(f\"{classes[label]} -> Patchified\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "embed_dim = 768 #D: number of feature / activation maps\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "    in_channels = 3 , \n",
    "    out_channels = embed_dim , \n",
    "    kernel_size = patch_size , \n",
    "    stride = patch_size , \n",
    "    padding = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(classes[label])\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image shape before: \" , image.shape)\n",
    "# expected shape for the Conv2d is (N , C , H , W)\n",
    "image = image.unsqueeze(0) \n",
    "print(\"image shape after\" , image.shape)\n",
    "\n",
    "img_conv = conv2d(image)\n",
    "print(\"image shape after Conv2D : \" , img_conv.shape)\n",
    "# output shape [batch_size , embed_dim , feature_map_height , feature_map_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random_indices = random.sample(range(0 , 758) , k=10)\n",
    "\n",
    "print(f\"showing random convolutional feature maps from indices : {random_indices}\" )\n",
    "\n",
    "fig , axs = plt.subplots(nrows = 1 , ncols=len(random_indices) , figsize=(12,12))\n",
    "\n",
    "for i,idx in enumerate(random_indices):\n",
    "    image_conv_feat_map = img_conv[: , idx , : , :]\n",
    "    \n",
    "    axs[i].imshow(image_conv_feat_map.squeeze().detach().numpy())\n",
    "    # detach takes a copy of the tensor that disconnected from the gradient graph \n",
    "    axs[i].set(xticklabels = [] , yticklabels = [] , xticks = [] ,\n",
    "               yticks = []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After turning the image into patch embedding , its time to flatten it\n",
    "\n",
    "print(f\"shape of the output of the conv : {img_conv.shape} -> [batch , embedding_dim , feature_map_height, feature_map_width]\")\n",
    "# what we want to flatten is the spatial dimension of the feature map\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "t = flatten(img_conv)\n",
    "print(t.shape) # this is flattening the whole tensor (1 , 768 * 14 * 14)\n",
    "\n",
    "flatten = nn.Flatten(start_dim= 2 , end_dim = 3 )\n",
    "t = flatten(img_conv)\n",
    "\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1 , 2, 0))\n",
    "plt.title(classes[label])\n",
    "plt.axis(False)\n",
    "print(f\"original image shape: {image.shape}\")\n",
    "\n",
    "image_conv = conv2d(image.unsqueeze(0)) # (N , C, H , W)\n",
    "print(f\"shape after convolution: {image_conv.shape}\")# 768 feature maps each of size 14 x 14\n",
    "\n",
    "image_flatten = flatten(image_conv)\n",
    "print(f\"shape after flatten: {image_flatten.shape}\") # 768 feature maps each of flattend size of 196\n",
    "\n",
    "print(\"desired shape : N x(P^2 * C) --> (196 , 768)\")\n",
    "\n",
    "image_final = image_flatten.permute(0 , 2 , 1)\n",
    "print(f\"Patch embedding final shape : {image_final.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample = image_final[: , : , 0]\n",
    "print(\"shape of this sample : \" , single_sample.shape) # 2D image to 1D embedding vector\n",
    "plt.figure(figsize=(20 , 20))\n",
    "plt.imshow(single_sample.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {image_final.shape}\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up everything till now in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    turns 2d input image into a 1D sequence learnable embedding vector\n",
    "    \n",
    "    Args:\n",
    "    in_channels(int) : Number of channels in the input image. (Default set to 3)\n",
    "    patch_size (int) : size of the patch (Defaults to 16)\n",
    "    embed_dim (int) : number of feature maps (Defaults to 768) \n",
    "    \"\"\"\n",
    "    def __init__(self ,\n",
    "                 in_channels: int = 3 ,\n",
    "                 patch_size : int = 16 , \n",
    "                 embed_dim: int = 768 ):\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels= in_channels , \n",
    "                           out_channels = embed_dim , \n",
    "                           kernel_size= patch_size , \n",
    "                           stride = patch_size , \n",
    "                           padding = 0)\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim = 2 , end_dim = 3)\n",
    "    \n",
    "    def forward(self , image):\n",
    "        height = image.shape[-1]\n",
    "        assert height % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {height}, patch size: {patch_size}\"\n",
    "        patched_image = self.conv2d(image)\n",
    "        flattened_image = self.flatten(patched_image)\n",
    "        \n",
    "        return flattened_image.permute(0 , 2 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embed_dim=768)\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input sizes\n",
    "random_input_image = ( 3, 224, 224)\n",
    "random_input_image_error = ( 3, 250, 250) # will error because image size is incompatible with patch_size\n",
    "\n",
    "# Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n",
    "summary(PatchEmbedding().to(device),\n",
    "        input_size=random_input_image, # try swapping this for \"random_input_image_error\"\n",
    "        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"patch_embedded_image shape \" , patch_embedded_image.shape)\n",
    "\n",
    "batch_size , embedding_dim = patch_embedded_image.shape[0] , patch_embedded_image.shape[-1]\n",
    "\n",
    "class_token = nn.Parameter(torch.ones((batch_size , 1 , embedding_dim)) ,\n",
    "                           requires_grad= True)# [batch_size, number_of_tokens, embedding_dimension]\n",
    "\n",
    "print(f\"class_token: {class_token[: , : , :10]}\")\n",
    "print(f\"class_token shape: {class_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_patch = torch.cat((class_token , patch_embedded_image) , \n",
    "                          dim = 1)\n",
    "\n",
    "print(f\"shape of updated patch embedding with class token: {updated_patch.shape}\")\n",
    "\n",
    "# before each feature map was represented with 196 tokens  , however now after adding the class token each will be represented by 197\n",
    "\n",
    "print(updated_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embedding\n",
    "\n",
    "Epos --> (N+1 X D) where N is the number of patches and the 1 is for class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_patch , updated_patch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_patches = int((height * width) / patch_size **2)\n",
    "\n",
    "embedding_dim = updated_patch.shape[2]\n",
    "\n",
    "position_embedding = nn.Parameter(\n",
    "    torch.ones( 1 , numb_patches +1 , embedding_dim),\n",
    "                        requires_grad= True        ) # learnable\n",
    "\n",
    "position_embedding[: , :10 , :10] , position_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_with_position = updated_patch + position_embedding\n",
    "patch_with_position , patch_with_position.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
